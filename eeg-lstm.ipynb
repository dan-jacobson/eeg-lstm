{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**\n",
    "\n",
    "We start out by reading in our data and formatting it appropriately.\n",
    "\n",
    "Define some helper functions to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our datatypes\n",
    "types1 = {\n",
    "    'Id': 'str', 'Fp1': 'int16', 'Fp2': 'int16', 'F7': 'int16', 'F3': 'int16', 'Fz': 'int16',\n",
    "    'F4': 'int16', 'F8': 'int16', 'FC5': 'int16', 'FC1': 'int16', 'FC2': 'int16', 'FC6': 'int16',\n",
    "    'T7': 'int16', 'C3': 'int16', 'Cz': 'int16', 'C4': 'int16', 'T8': 'int16', 'TP9': 'int16',\n",
    "    'CP5': 'int16', 'CP1': 'int16', 'CP2': 'int16', 'CP6': 'int16', 'TP10': 'int16', 'P7': 'int16',\n",
    "    'P3': 'int16', 'Pz': 'int16', 'P4': 'int16', 'P8': 'int16', 'PO9': 'int16', 'O1': 'int16',\n",
    "    'Oz': 'int16', 'O2': 'int16', 'PO10': 'int16'}\n",
    "\n",
    "types2 = {\n",
    "    'Id': 'str', 'HandStart': 'int8', 'FirstDigitTouch': 'int8', 'BothStartLoadPhase': 'int8', 'LiftOff': 'int8', \n",
    "    'Replace': 'int8', 'BothReleased': 'int8'}\n",
    "\n",
    "# define our dependent variables to predict\n",
    "dep_vars = ['HandStart','FirstDigitTouch','BothStartLoadPhase','LiftOff','Replace','BothReleased']\n",
    "\n",
    "# define our read data function\n",
    "def read_data(subjects, series, path):\n",
    "    df = None\n",
    "    labels = None\n",
    "    \n",
    "    # if we are passed individual subjects or series, encode as iterables\n",
    "    if isinstance(subjects,int):\n",
    "         subjects = [subjects]\n",
    "    \n",
    "    if isinstance(series,int):\n",
    "        series = [series]\n",
    "    \n",
    "    # for each passed subject and series, build a pandas dataframe\n",
    "    for i in subjects:\n",
    "        print('Loading',path,'data for subject',i)\n",
    "        for j in series:\n",
    "            c_df = pd.read_csv(f'../input/{path}/subj{i}_series{j}_data.csv', dtype=types1)\n",
    "            df = c_df if df is None else df.append(c_df, ignore_index = True)\n",
    "            \n",
    "            if path == 'train':\n",
    "                c_label = pd.read_csv(f'../input/{path}/subj{i}_series{j}_events.csv', dtype=types2)\n",
    "                labels = c_label if labels is None else labels.append(c_label, ignore_index = True)\n",
    "            else: labels = None    \n",
    "            \n",
    "       \n",
    "    return df, labels\n",
    "\n",
    "# optional function to add \"subject\" and \"series\" as columns to our dataframe\n",
    "def add_subs_and_ser(df):\n",
    "    df.insert(0,'subject', df.id.str.extract(r'(\\d)'))\n",
    "    df.insert(1,'series', df.id.str.extract(r'[^\\d]*[\\d]+[^\\d]+([\\d]+)'))\n",
    "    return df\n",
    "\n",
    "# formating function to strip out the \"id\" column from label data\n",
    "def format_y(df):\n",
    "    df = df.drop(columns='id', axis=1)\n",
    "    return df\n",
    "\n",
    "# formating function to strip out the \"id\" column from signal data. Options to encode additional signal \n",
    "# preprocessing in this function\n",
    "def format_X(df):\n",
    "    df = df.drop(columns='id', axis=1)\n",
    "    columns = df.columns      \n",
    "    \n",
    "    #Preprocessing    \n",
    "    scaler = StandardScaler() \n",
    "    df =np.asarray(df.astype(float))\n",
    "    df = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    \n",
    "    ## additional preprocessing could go here\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate this model, I've set it up to train on Subject 4. It uses series 1-7 as the training set and series 8 as the validation set. To train, simply shift-enter your way through, and enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration training dataset parameters\n",
    "subjects = [4]\n",
    "series_train = range(1,8)\n",
    "series_test = [8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data for subject 4\n"
     ]
    }
   ],
   "source": [
    "# reads training data for Subject 4, training on series 1-7\n",
    "\n",
    "X_train, y_train = read_data(subjects, series_train, path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data for subject 4\n"
     ]
    }
   ],
   "source": [
    "# splits series 8 to use as a validation set for the model\n",
    "\n",
    "X_test, y_test = read_data(subjects, series_test, path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formats the training and validation datasets, preparing them to be fed into the model\n",
    "\n",
    "X_train = format_X(X_train)\n",
    "y_train = format_y(y_train)\n",
    "\n",
    "X_test = format_X(X_test)\n",
    "y_test = format_y(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the model**\n",
    "\n",
    "Now that we have our data loaded up, let's construct some of the hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model parameters ###\n",
    "\n",
    "n_features = 32  # how many channels of eeg in each sample\n",
    "event_types = 6 #len(set(y))  # how many different event types\n",
    "look_back = 35 #add a rear-facing window\n",
    "downsample = 15 #choose every 15th datapoint\n",
    "# l1 = 0 #used for regularization in Haunke architecture\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to feed data to our LSTM, we need a 3D dataset. We use a\n",
    "# \"look_back\" window to add the third dimension to the model.\n",
    "\n",
    "def add_lookback(dataset,labels,look_back):\n",
    "    dataX = []\n",
    "    dataY = labels[look_back:]\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        dataX.append(dataset[i:(i+look_back), ])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are shift-entering through the script, this adds the lookback window\n",
    "# to the train and validation datasets\n",
    "\n",
    "X_train, y_train = add_lookback(\n",
    "                    X_train.iloc[::downsample].values,\n",
    "                    y_train.iloc[::downsample].values,\n",
    "                    look_back=look_back)\n",
    "\n",
    "X_test, y_test = add_lookback(\n",
    "                    X_test.iloc[::downsample].values,\n",
    "                    y_test.iloc[::downsample].values,\n",
    "                    look_back=look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras layers to use in subsequent models\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import AveragePooling1D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks=[EarlyStopping(monitor=\"acc\", \n",
    "                         verbose=1, \n",
    "                         patience=6, \n",
    "                         restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define an LSTM model ###\n",
    "\n",
    "def simple_LSTM(look_back, n_features):\n",
    "    # this implements a simple LSTM network that can train across all subjects, all labels.\n",
    "    # network has 100 neurons and dropout. In limited optimization, 10-15 epochs worked pretty well\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=False, input_shape=(look_back, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(LSTM(100)) dramatically worse results \n",
    "    model.add(Dense(event_types, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment below to continue to shift-enter through ###\n",
    "\n",
    "#model = simple_LSTM(look_back=look_back, n_features=n_features)\n",
    "#model.fit(X_train, y_train, batch_size=32, epochs=epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per Haunke, et al, 2018\n",
    "\n",
    "def hauke_cnn(look_back, n_features,l1):\n",
    "    # I attempted to implement a CNN with layers operating across both the temporal and spatial axes.\n",
    "    # Unfortunately the model wasn't working well and I couldn't get it outperforming my simple \n",
    "    # LSTM in the given time.\n",
    "    \n",
    "    input_shape = (look_back, n_features)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(40, 30, activation=\"relu\", kernel_regularizer=regularizers.l1(l1), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Conv1D(40, n_features, activation=\"relu\", kernel_regularizer=regularizers.l1(l1), padding=\"valid\"))\n",
    "    model.add(AveragePooling1D(1, strides=(15)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(80, activation=\"relu\"))\n",
    "    model.add(Dense(event_types, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "#model.fit(X_train, y_train, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31dc404cf719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a9c9c405c5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function is called inside the Kaggle kernel to train the model, test it on the Kaggle \"test set\",\n",
    "### and then submit it.\n",
    "\n",
    "def make_submission(subjects, batch_size):\n",
    "    # initialize results lists\n",
    "    Results = []\n",
    "    subj_ids = []\n",
    "    \n",
    "    ### call the full stack on each subject. For each subject we load data, train the model, predict output,\n",
    "    ### and append results\n",
    "    for i in subjects:\n",
    "        print(\"Training on subject\",i)\n",
    "        \n",
    "        ### start by training the model\n",
    "        model = simple_LSTM(look_back=look_back, n_features=n_features)\n",
    "        \n",
    "        ### for each subject, load all training data\n",
    "        series_train = range(1,9)\n",
    "        \n",
    "        ### load training data for our subject\n",
    "        X_train, y_train = read_data(i, series_train, path='train')\n",
    "        \n",
    "        ### process our data\n",
    "        X_train = format_X(X_train)  \n",
    "        y_train = format_y(y_train)\n",
    "        \n",
    "        ### add our lookback window\n",
    "        X_train, y_train = add_lookback(X_train.iloc[::downsample].values,\n",
    "                                        y_train.iloc[::downsample].values,\n",
    "                                        look_back=look_back)\n",
    "        \n",
    "        \n",
    "        ### crop training data to fit batch size\n",
    "        train_crop_to_batch = (((len(X_train)) // batch_size) * batch_size)\n",
    "        \n",
    "        ### fit model to training data\n",
    "        model.fit(X_train[0:train_crop_to_batch], y_train[0:train_crop_to_batch], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        \n",
    "        ### move on to predicting results. we use series 9 and 10, the Kaggle \"test\"\n",
    "        ### dataset\n",
    "        for j in range(9,11):\n",
    "            \n",
    "            ### read in test data\n",
    "            X_test, y_test = read_data(i, j, path='test')\n",
    "            y_test = X_test['id'] # save the 'id' column for later\n",
    "            \n",
    "            ### format test data\n",
    "            X_test = format_X(X_test)\n",
    "            \n",
    "            ### record the length of the test set for future reference\n",
    "            test_length = len(X_test)\n",
    "            #print(\"length of test dataset is\",test_length) #useful for debugging\n",
    "            \n",
    "            # add look_back window to test data so we can predict it\n",
    "            X_test, y_test_smol = add_lookback(X_test.iloc[::downsample].values,\n",
    "                                        y_test.iloc[::downsample].values,\n",
    "                                        look_back=look_back)\n",
    "            #print(\"length of test dataset post-lookback is\",len(X_test)) #useful for debugging\n",
    "            \n",
    "            ### crop predicting data to fit batch size\n",
    "            test_crop_to_batch = ((len(X_test) // batch_size) * batch_size)\n",
    "            #print(\"length of test dataset cropped-to-batch is\",test_crop_to_batch) #useful for debugging\n",
    "            \n",
    "            ### predict results on the provided test data\n",
    "            result = model.predict(X_test[0:test_crop_to_batch], batch_size=batch_size)\n",
    "            result = np.array(result)\n",
    "            #print(\"length of result is\",len(result)) #useful for debugging\n",
    "            \n",
    "            ### because of our rear window, we have to pad out the beginning of the results with 0's\n",
    "            for _ in range((look_back)*downsample):\n",
    "                Results.append(np.array([[0,0,0,0,0,0]]))\n",
    "            \n",
    "            #print(\"length of Results with initial zero padding is\",len(Results)) #useful for debugging\n",
    "            \n",
    "            ### because we downsampled, we have to fill in the missing values. A simple solution is to\n",
    "            ### expand the results we have to fill the space.\n",
    "            result = np.repeat(result,downsample, axis=0)\n",
    "            for row in result:\n",
    "                Results.append(np.array([row]))\n",
    "            #print(\"length of result after multiplying is\",len(result)) #useful for debugging\n",
    "            \n",
    "            for _ in range(test_length-len(result)-((look_back)*downsample)):\n",
    "                Results.append(np.array([[0,0,0,0,0,0]]))\n",
    "            #print(\"length of final 0s padding is\",(test_length-len(result)-(look_back*downsample))) #useful for debugging\n",
    "            \n",
    "            subj_ids.append(y_test)\n",
    "            \n",
    "            ### calculate the total length of our results and our subject ids to make sure\n",
    "            ### they match\n",
    "            length_results = len(np.concatenate(Results))\n",
    "            length_ids = len(np.concatenate(subj_ids))\n",
    "            \n",
    "            if length_results == length_ids:\n",
    "                print(\"predictions for Series\",j,\"are the correct length!\")\n",
    "            else: print(\"Woops! Length of results is\",length_results,\"but ids length is\",length_ids)\n",
    "            \n",
    "                \n",
    "    print(\"printing submissions\")\n",
    "    submission_name = \"simple_LSTM.csv\"\n",
    "    \n",
    "    ### concatenate everything into a submission dataframe\n",
    "    submission = pd.DataFrame(columns=dep_vars, data=np.concatenate(Results), index=np.concatenate(subj_ids))\n",
    "    \n",
    "    ### sort the dataframe into the correct order for submission\n",
    "    submission = pd.concat([submission[submission.index.str.contains('series9')],\n",
    "                            submission[submission.index.str.contains('series10')]])\n",
    "    \n",
    "    ### write the submission dataframe to csv\n",
    "    submission.to_csv(submission_name,index_label=\"id\",float_format='%.3f')\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on subject 1\n",
      "Loading train data for subject 1\n",
      "Epoch 1/30\n",
      "94500/94500 [==============================] - 11s 119us/step - loss: 0.2168 - acc: 0.4336\n",
      "Epoch 2/30\n",
      "94500/94500 [==============================] - 9s 98us/step - loss: 0.1418 - acc: 0.5418\n",
      "Epoch 3/30\n",
      "94500/94500 [==============================] - 9s 98us/step - loss: 0.1202 - acc: 0.5657\n",
      "Epoch 4/30\n",
      "94500/94500 [==============================] - 9s 98us/step - loss: 0.1110 - acc: 0.5788\n",
      "Epoch 5/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.1076 - acc: 0.5743\n",
      "Epoch 6/30\n",
      "94500/94500 [==============================] - 9s 98us/step - loss: 0.1044 - acc: 0.5820\n",
      "Epoch 7/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0993 - acc: 0.5928\n",
      "Epoch 8/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0946 - acc: 0.6104\n",
      "Epoch 9/30\n",
      "94500/94500 [==============================] - 9s 98us/step - loss: 0.0883 - acc: 0.5995\n",
      "Epoch 10/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0879 - acc: 0.5893\n",
      "Epoch 11/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0878 - acc: 0.6007\n",
      "Epoch 12/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0915 - acc: 0.5982\n",
      "Epoch 13/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0833 - acc: 0.6088\n",
      "Epoch 14/30\n",
      "94500/94500 [==============================] - 9s 97us/step - loss: 0.0805 - acc: 0.6081\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00014: early stopping\n",
      "Loading test data for subject 1\n",
      "predictions for Series 9 are the correct length!\n",
      "Loading test data for subject 1\n",
      "predictions for Series 10 are the correct length!\n",
      "Training on subject 2\n",
      "Loading train data for subject 2\n",
      "Epoch 1/30\n",
      "114000/114000 [==============================] - 12s 105us/step - loss: 0.1848 - acc: 0.2779\n",
      "Epoch 2/30\n",
      "114000/114000 [==============================] - 11s 96us/step - loss: 0.1306 - acc: 0.3716\n",
      "Epoch 3/30\n",
      "114000/114000 [==============================] - 11s 98us/step - loss: 0.1077 - acc: 0.3857\n",
      "Epoch 4/30\n",
      "114000/114000 [==============================] - 11s 98us/step - loss: 0.0960 - acc: 0.3874\n",
      "Epoch 5/30\n",
      "114000/114000 [==============================] - 11s 96us/step - loss: 0.0872 - acc: 0.4031\n",
      "Epoch 6/30\n",
      "114000/114000 [==============================] - 11s 96us/step - loss: 0.0793 - acc: 0.3955\n",
      "Epoch 7/30\n",
      "114000/114000 [==============================] - 11s 96us/step - loss: 0.0757 - acc: 0.3960\n",
      "Epoch 8/30\n",
      "114000/114000 [==============================] - 11s 96us/step - loss: 0.0690 - acc: 0.4031\n",
      "Epoch 9/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0649 - acc: 0.4113\n",
      "Epoch 10/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0681 - acc: 0.4069\n",
      "Epoch 11/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0597 - acc: 0.4149\n",
      "Epoch 12/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0581 - acc: 0.4153\n",
      "Epoch 13/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0581 - acc: 0.4241\n",
      "Epoch 14/30\n",
      "114000/114000 [==============================] - 12s 102us/step - loss: 0.0541 - acc: 0.4162\n",
      "Epoch 15/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0573 - acc: 0.4075\n",
      "Epoch 16/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0507 - acc: 0.4161\n",
      "Epoch 17/30\n",
      "114000/114000 [==============================] - 11s 94us/step - loss: 0.0495 - acc: 0.4106\n",
      "Epoch 18/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0498 - acc: 0.4120\n",
      "Epoch 19/30\n",
      "114000/114000 [==============================] - 11s 95us/step - loss: 0.0502 - acc: 0.4187\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00019: early stopping\n",
      "Loading test data for subject 2\n",
      "predictions for Series 9 are the correct length!\n",
      "Loading test data for subject 2\n",
      "predictions for Series 10 are the correct length!\n",
      "Training on subject 3\n",
      "Loading train data for subject 3\n",
      "Epoch 1/30\n",
      "89000/89000 [==============================] - 10s 108us/step - loss: 0.2715 - acc: 0.1651\n",
      "Epoch 2/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.2202 - acc: 0.2841\n",
      "Epoch 3/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.2070 - acc: 0.3338\n",
      "Epoch 4/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1816 - acc: 0.3763\n",
      "Epoch 5/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1744 - acc: 0.4014\n",
      "Epoch 6/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1614 - acc: 0.3983\n",
      "Epoch 7/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1566 - acc: 0.4008\n",
      "Epoch 8/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1558 - acc: 0.4077\n",
      "Epoch 9/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1541 - acc: 0.4083\n",
      "Epoch 10/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1494 - acc: 0.4067\n",
      "Epoch 11/30\n",
      "89000/89000 [==============================] - 9s 96us/step - loss: 0.1492 - acc: 0.4013\n",
      "Epoch 12/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1423 - acc: 0.4121\n",
      "Epoch 13/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1468 - acc: 0.4059\n",
      "Epoch 14/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1440 - acc: 0.4105\n",
      "Epoch 15/30\n",
      "89000/89000 [==============================] - 9s 97us/step - loss: 0.1474 - acc: 0.4045\n",
      "Epoch 16/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1435 - acc: 0.4164\n",
      "Epoch 17/30\n",
      "89000/89000 [==============================] - 8s 94us/step - loss: 0.1407 - acc: 0.4103\n",
      "Epoch 18/30\n",
      "89000/89000 [==============================] - 8s 94us/step - loss: 0.1428 - acc: 0.4203\n",
      "Epoch 19/30\n",
      "89000/89000 [==============================] - 8s 94us/step - loss: 0.1384 - acc: 0.4248\n",
      "Epoch 20/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1314 - acc: 0.4344\n",
      "Epoch 21/30\n",
      "89000/89000 [==============================] - 8s 95us/step - loss: 0.1296 - acc: 0.4350\n",
      "Epoch 22/30\n",
      "11500/89000 [==>...........................] - ETA: 7s - loss: 0.1337 - acc: 0.4398"
     ]
    }
   ],
   "source": [
    "# call our function inside Kaggle to submit it.\n",
    "make_submission(range(1,13),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
