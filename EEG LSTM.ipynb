{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nfrom sklearn.preprocessing import StandardScaler\n\npath = 'train'","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Data**\n\nWe start out by reading in our data and formatting it appropriately."},{"metadata":{"trusted":true},"cell_type":"code","source":"types1 = {\n    'Id': 'str', 'Fp1': 'int16', 'Fp2': 'int16', 'F7': 'int16', 'F3': 'int16', 'Fz': 'int16',\n    'F4': 'int16', 'F8': 'int16', 'FC5': 'int16', 'FC1': 'int16', 'FC2': 'int16', 'FC6': 'int16',\n    'T7': 'int16', 'C3': 'int16', 'Cz': 'int16', 'C4': 'int16', 'T8': 'int16', 'TP9': 'int16',\n    'CP5': 'int16', 'CP1': 'int16', 'CP2': 'int16', 'CP6': 'int16', 'TP10': 'int16', 'P7': 'int16',\n    'P3': 'int16', 'Pz': 'int16', 'P4': 'int16', 'P8': 'int16', 'PO9': 'int16', 'O1': 'int16',\n    'Oz': 'int16', 'O2': 'int16', 'PO10': 'int16'}\n\ntypes2 = {\n    'Id': 'str', 'HandStart': 'int8', 'FirstDigitTouch': 'int8', 'BothStartLoadPhase': 'int8', 'LiftOff': 'int8', \n    'Replace': 'int8', 'BothReleased': 'int8'}\n\ndep_vars = ['HandStart','FirstDigitTouch','BothStartLoadPhase','LiftOff','Replace','BothReleased']\n    \ndef read_data(subjects, series, path):\n    df = None\n    labels = None\n    \n    if isinstance(subjects,int):\n         subjects = [subjects]\n    \n    if isinstance(series,int):\n        series = [series]\n    \n    for i in subjects:\n        print('Loading',path,'data for subject',i)\n        for j in series:\n            c_df = pd.read_csv(f'../input/{path}/subj{i}_series{j}_data.csv', dtype=types1)\n            df = c_df if df is None else df.append(c_df, ignore_index = True)\n            \n            if path == 'train':\n                c_label = pd.read_csv(f'../input/{path}/subj{i}_series{j}_events.csv', dtype=types2)\n                labels = c_label if labels is None else labels.append(c_label, ignore_index = True)\n            else: labels = None    \n            \n       \n    return df, labels\n    \ndef add_subs_and_ser(df):\n    df.insert(0,'subject', df.id.str.extract(r'(\\d)'))\n    df.insert(1,'series', df.id.str.extract(r'[^\\d]*[\\d]+[^\\d]+([\\d]+)'))\n    return df\n\ndef format_y(df):\n    df = df.drop(columns='id', axis=1)\n    return df\n\ndef format_X(df):\n    df = df.drop(columns='id', axis=1)\n    columns = df.columns      \n    \n    #Preprocessing    \n    scaler = StandardScaler() \n    df =np.asarray(df.astype(float))\n    df = scaler.fit_transform(df)\n    df = pd.DataFrame(df, columns=columns) \n    \n    return df","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to demonstrate this model, I've set it up to train on Subject 4. It uses series 1-7 as the training set and series 8 as the validation set. Simply to shift-enter your way through, and enjoy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# training dataset parameters\nsubjects = [4]\nseries_train = range(1,8)\nseries_test = [8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reads training data for Subject 4, training on series 1-7\n\nX_train, y_train = read_data(subjects, series_train, path='train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splits series 8 to use as a validation set for the model\n\nX_test, y_test = read_data(subjects, series_test, path='train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# formats the training and validation datasets, preparing them to be fed into the model\n\nX_train = format_X(X_train)\ny_train = format_y(y_train)\n\nX_test = format_X(X_test)\ny_test = format_y(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build the model**\n\nNow that we have our data (mostly) loaded up, let's construct some of the hyperparameters for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Model parameters ###\n\nn_features = 32  # how many channels of eeg in each sample\nevent_types = 6 #len(set(y))  # how many different event types\nlook_back = 35 #add a rear-facing window\ndownsample = 15 #choose every 15th datapoint\n# l1 = 0 #used for regularization in Haunke architecture\nepochs = 10","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to feed data to our LSTM, we need a 3D dataset. We use a\n# \"look_back\" window to add the third dimension to the model.\n\ndef add_lookback(dataset,labels,look_back):\n    dataX = []\n    dataY = labels[look_back:]\n    for i in range(len(dataset)-look_back):\n        dataX.append(dataset[i:(i+look_back), ])\n    return np.array(dataX), np.array(dataY)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if you are shift-entering through the script, this adds the lookback window\n# to the train and validation datasets\n\nX_train, y_train = add_lookback(\n                    X_train.iloc[::downsample].values,\n                    y_train.iloc[::downsample].values,\n                    look_back=look_back)\n\nX_test, y_test = add_lookback(\n                    X_test.iloc[::downsample].values,\n                    y_test.iloc[::downsample].values,\n                    look_back=look_back)","execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-193ad873f502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train, y_train = add_lookback(\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     look_back=look_back)\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout, LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.pooling import AveragePooling1D\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping\n\ncallbacks=[EarlyStopping(monitor=\"acc\", \n                         verbose=1, \n                         patience=6, \n                         restore_best_weights=True)]","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_LSTM(look_back, n_features):\n    # this implements a simple LSTM network that can train across all subjects, all labels.\n    # network has 100 neurons and dropout. In limited optimization, 10-15 epochs work pretty well\n    \n    model = Sequential()\n    model.add(LSTM(50, return_sequences=False, input_shape=(look_back, n_features)))\n    model.add(Dropout(0.5))\n    # model.add(LSTM(100)) dramatically worse results \n    model.add(Dense(event_types, activation='sigmoid'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    return model\n\n### uncomment below to continue to shift-enter through ###\n\n#model = simple_LSTM(look_back=look_back, n_features=n_features)\n#model.fit(X_train, y_train, batch_size=32, epochs=epochs, callbacks=callbacks)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# per Haunke, et al, 2018\n\ndef hauke_cnn(look_back, n_features,l1):\n    # I attempted to implement a CNN with layers across both the temporal and spatial axes.\n    # Unfortunately couldn't get it outperforming my simple LSTM in the given time.\n    \n    input_shape = (look_back, n_features)\n\n    model = Sequential()\n    model.add(Conv1D(40, 30, activation=\"relu\", kernel_regularizer=regularizers.l1(l1), padding=\"same\", input_shape=input_shape))\n    model.add(Conv1D(40, n_features, activation=\"relu\", kernel_regularizer=regularizers.l1(l1), padding=\"valid\"))\n    model.add(AveragePooling1D(1, strides=(15)))\n    model.add(Flatten())\n    model.add(Dense(80, activation=\"relu\"))\n    model.add(Dense(event_types, activation=\"softmax\"))\n    \n    model.compile(loss=\"categorical_crossentropy\", \n                  optimizer=\"adam\", \n                  metrics=[\"acc\"])\n    return model\n\n#model.fit(X_train, y_train, batch_size=32, epochs=epochs)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: %.2f%%\" % (score[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### this function is called to train the model, test it on the Kaggle \"test set\",\n### and then submit it.\n\ndef make_submission(subjects, batch_size):\n    Results = []\n    subj_ids = []\n    \n    ### call the full stack on each subject. For each, train data, test data, append results\n    for i in subjects:\n        print(\"Training on subject\",i)\n        \n        ### start by training the model\n        model = simple_LSTM(look_back=look_back, n_features=n_features)\n        \n        ### for each subject, load all training data\n        series_train = range(1,9)\n        \n        ### load training data for our subject\n        X_train, y_train = read_data(i, series_train, path='train')\n        \n        ### process our data\n        X_train = format_X(X_train)\n        \n        y_train = format_y(y_train)\n        \n        ### add our lookback window\n        X_train, y_train = add_lookback(X_train.iloc[::downsample].values,\n                                        y_train.iloc[::downsample].values,\n                                        look_back=look_back)\n        \n        \n        ### crop training data to fit batch size\n        train_crop_to_batch = (((len(X_train)) // batch_size) * batch_size)\n        \n        ### fit model to training data\n        model.fit(X_train[0:train_crop_to_batch], y_train[0:train_crop_to_batch], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n        \n        ### move on to predicting results. we use series 9 and 10, the Kaggle \"test\"\n        ### dataset\n        for j in range(9,11):\n            \n            ### read in test data\n            X_test, y_test = read_data(i, j, path='test')\n            y_test = X_test['id'] # save the 'id' column for later\n            \n            ### format test data\n            X_test = format_X(X_test)\n            \n            ### record the length of the test set for future reference\n            test_length = len(X_test)\n            #print(\"length of test dataset is\",test_length) #useful for debugging\n            \n            # add look_back window to test data so we can predict it\n            X_test, y_test_smol = add_lookback(X_test.iloc[::downsample].values,\n                                        y_test.iloc[::downsample].values,\n                                        look_back=look_back)\n            #print(\"length of test dataset post-lookback is\",len(X_test)) #useful for debugging\n            \n            ### crop predicting data to fit batch size\n            test_crop_to_batch = ((len(X_test) // batch_size) * batch_size)\n            #print(\"length of test dataset cropped-to-batch is\",test_crop_to_batch) #useful for debugging\n            \n            ### predict results on the provided test data\n            result = model.predict(X_test[0:test_crop_to_batch], batch_size=batch_size)\n            result = np.array(result)\n            #print(\"length of result is\",len(result)) #useful for debugging\n            \n            ### because of our rear window, we have to pad out the beginning of the results with 0's\n            for _ in range((look_back)*downsample):\n                Results.append(np.array([[0,0,0,0,0,0]]))\n            \n            #print(\"length of Results with initial zero padding is\",len(Results)) #useful for debugging\n            \n            ### because we downsampled, we have to fill in the missing values.\n            ### simply expand the results we have to fill the space.\n            result = np.repeat(result,downsample, axis=0)\n            for row in result:\n                Results.append(np.array([row]))\n            #print(\"length of result after multiplying is\",len(result)) #useful for debugging\n            \n            for _ in range(test_length-len(result)-((look_back)*downsample)):\n                Results.append(np.array([[0,0,0,0,0,0]]))\n            #print(\"length of final 0s padding is\",(test_length-len(result)-(look_back*downsample))) #useful for debugging\n            \n            subj_ids.append(y_test)\n            \n            ### calculate the total length of our results and our subject ids to make sure\n            ### they match\n            length_results = len(np.concatenate(Results))\n            length_ids = len(np.concatenate(subj_ids))\n            \n            if length_results == length_ids:\n                print(\"predictions for Series\",j,\"are the correct length!\")\n            else: print(\"Woops! Length of results is\",length_results,\"but ids length is\",length_ids)\n            \n                \n    print(\"printing submissions\")\n    submission_name = \"simple_LSTM.csv\"\n    \n    ### concatinate everything into a submission dataframe\n    submission = pd.DataFrame(columns=dep_vars, data=np.concatenate(Results), index=np.concatenate(subj_ids))\n    \n    ### sort the dataframe into the correct order for submission\n    submission = pd.concat([submission[submission.index.str.contains('series9')],\n                            submission[submission.index.str.contains('series10')]])\n    \n    ### write the submission dataframe to csv\n    submission.to_csv(submission_name,index_label=\"id\",float_format='%.3f')\n    \n    return submission\n            \n    \n    ","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_submission([1,2,3,4,5,6,7,8,9,10,11,12],300)","execution_count":null,"outputs":[{"output_type":"stream","text":"Training on subject 1\nLoading train data for subject 1\nEpoch 1/10\n60300/94500 [==================>...........] - ETA: 8s - loss: 0.2464 - acc: 0.2710","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}